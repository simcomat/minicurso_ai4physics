{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23165e7",
   "metadata": {},
   "source": [
    "# Regressão\n",
    "\n",
    "A tarefa de **regressão** consiste em predizer **valores** para determinados **objetos**. Vamos contextualizar de maneira operacional esses objetos e valores através de seus tipos de variáveis: em geral um valor é um número real <code>(float)</code> ou um inteiro <code>(int)</code>; já o objeto a ser regredido pode ser um **vetor de features** - aqui vetor tem o mesmo sentido de uma variável lista <code>(lst)</code> ou uma tupla <code>(tuple)</code>. \n",
    "\n",
    "Diferentes **algoritmos de regressão** podem ser usados para realizar essa tarefa de, dada uma entrada, gerar uma saída (valor predito para aquele objeto).\n",
    "\n",
    "Define-se a regressão como uma tarefa de **aprendizado de máquina supervisionado**, isto é, os algoritmos de regressão precisam ser treinados com pares objeto-valor tidos como associações corretas. Somente após o treinamento é que o algoritmo de regressão estará apto a predizer valores para novas entradas (das quais não se sabe o valor a princípio) - diz-se que  algoritmo aprendeu o padrão nos dados de treinamento e agora pode ser usado para predizr o valor de saída para novas entradas.\n",
    "\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0edad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No google colab é preciso atualizar a versão do matplotlib para gerar alguns detalhes nos gráficos\n",
    "#!pip install matplotlib --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7afc3",
   "metadata": {},
   "source": [
    "## 1 - IArpi Data Set\n",
    "\n",
    "#### Descrição geral:\n",
    "Três diferentes objetos são postos a se mover em um plano inclinado devido a ação da gravidade. Atributos cinemáticos do movimento dos corpos são coletados. Pretende-se estabelecer uma relação entre esses atributos e cada tipo de objeto.\n",
    "\n",
    "#### Objetivo:\n",
    "O problema consiste na predição da velocidade média de três objetos (esfera, cilindro e aro) a partir da sua altura inicial em um plano inclinado a um determinado ângulo. O objetivo é introduzir técnicas de IA para cursos de graduação de física onde o experimento do plano inclinado é amplamente estudado (de maneira teórica e em laboratório).\n",
    "\n",
    "\n",
    "#### Features (variáveis de entrada):\n",
    "As features foram determinadas experimentalmente:\n",
    "- Ângulo: ângulo de inclinação do plano [°]\n",
    "- Altura: de partida do objeto [m]\n",
    "- Tipo de objeto (esfera, cilindro, aro)\n",
    "\n",
    "#### Alvo (valor de saída):\n",
    "- Velocidade Média: velocidade média determinada pela distância/tempo [m/s]\n",
    "\n",
    "#### Referências:\n",
    "- https://github.com/simcomat/IArpi\n",
    "- Ferreira, H., Almeida Junior, E. F., Espinosa-García, W., Novais, E., Rodrigues, J. N. B., & Dalpian, G. M. (2022). Introduzindo aprendizado de máquina em cursos de física: o caso do rolamento no plano inclinado. In Revista Brasileira de Ensino de Física (Vol. 44). FapUNIFESP (SciELO). https://doi.org/10.1590/1806-9126-rbef-2022-0214 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be16f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Para trabalhar com dados na forma de tabela\n",
    "import numpy as np  # Para trabalhar com vetores e matrizes\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # Separação treino/teste\n",
    "from sklearn.preprocessing import MinMaxScaler         # Escalonador\n",
    "\n",
    "# Algoritmos de Regressão\n",
    "from sklearn.linear_model import LinearRegression      # Regressao Linear\n",
    "from sklearn.svm import SVR                            # Regressão por Máquina de Vetor Suporte\n",
    "from sklearn.tree import DecisionTreeRegressor         # Regressão por Árvore de Decisão\n",
    "from sklearn.neighbors import KNeighborsRegressor      # k-vizinhos mais próximos (KNN)\n",
    "from sklearn.ensemble import RandomForestRegressor     # RandomForest\n",
    "from sklearn.ensemble import GradientBoostingRegressor # GradientBoosting\n",
    "from sklearn.neural_network import MLPRegressor        # Multilayer Perceptron\n",
    "\n",
    "# Validação cruzada\n",
    "from sklearn.model_selection import KFold            # Para separar os dados em k folds na regressao\n",
    "from sklearn.model_selection import cross_validate   # Para rodar treinamento e teste sobre kfolds\n",
    "from sklearn import preprocessing          # Auxilia na transformação dos dados (passo 3)\n",
    "from sklearn.pipeline import make_pipeline # Permite realizar uma sequência de processamentos\n",
    "\n",
    "# Métricas de desempenho\n",
    "from sklearn.metrics import mean_absolute_error, r2_score       # Métricas de Regressão\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# Módulos para plotar gráficos e ajustar formatação dos mesmos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# Para não aparecer avisos de warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros gerais para os gráficos\n",
    "# Definição dos tamanhos de fontes e ticks dos gráficos\n",
    "fsize = 12\n",
    "tsize = 10\n",
    "major = 5.0\n",
    "minor = 3.0\n",
    "\n",
    "style = 'default'\n",
    "plt.style.use(style)\n",
    "\n",
    "#plt.rcParams['text.usetex'] = True  # Para usar fonte tex (precisa instalar o tex antes)\n",
    "plt.rcParams['font.size'] = fsize      \n",
    "plt.rcParams['legend.fontsize'] = tsize\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.size'] = major\n",
    "plt.rcParams['xtick.minor.size'] = minor\n",
    "plt.rcParams['ytick.major.size'] = major\n",
    "plt.rcParams['ytick.minor.size'] = minor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcb4a3",
   "metadata": {},
   "source": [
    "### 1 - Primeiro passo: carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_dados = pd.read_csv('https://raw.githubusercontent.com/simcomat/IArpi/main/datasets/rolling.csv', sep=';') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5580873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_dados.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c73a47",
   "metadata": {},
   "source": [
    "### 1 - Segundo passo: separação de dados\n",
    "\n",
    "Os algoritmos de regressão não estão preparados para receber um nome (string) como entrada. Então, apenas fornecer o nome do objeto (esfera, cilindro, aro) para o modelo não irá funcionar. Precisamos primeiro converter esses nomes em uma representação numérica. Para isso vamos usar o **OneHot Encoding**. Nessa representação, cada valor do atributo objeto torna-se uma característica única (feature). Para 3 valores de objetos teremos 3 novas colunas com valores binários (verdadeiro ou falso, 1 ou 0) representando cada objeto, por exemplo, se seguirmos (esfera, cilindro, aro) uma esfera será representada pela tupla (1,0,0) enquanto um aro será por (0,0,1). Fisicamente, misturas não seriam possiveis como uma esfera-aro (1,0,1), mas em outras situações essa técnica pode ser usada para representar a presença de mais um objeto (como duas palavras diferentes em uma mesma frase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = pd.get_dummies(tabela_dados.Objeto, prefix='objeto') # One hot encoding\n",
    "tabela_dados = tabela_dados.join(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tabela_dados[['Altura (m)','Ângulo (°)', 'objeto_aro', 'objeto_cilindro','objeto_esfera']] \n",
    "y = tabela_dados['Velocidade Média (m/s)'] # Atributo alvo\n",
    "\n",
    "# Dividindo conjunto de treinamento e conjunto de teste\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54b1cb",
   "metadata": {},
   "source": [
    "### 1 - Terceiro passo: transformação dos dados\n",
    "\n",
    "**OBS:** perceba que nem sempre a transformação ocorre apenas no terceiro passo. Tivemos que fazer uma transformação usando o One Hot Encoding (OHE) no segundo passo. A troca da ordem neste caso foi feita para facilitar a etapa de separação (não ter que filtrar a coluna objeto após transformar via OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0624d2b",
   "metadata": {},
   "source": [
    "### 1 - Quarto passo: treinando os algoritmos de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressao Linear\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_scaled,y_train)\n",
    "\n",
    "# KNN Regressor\n",
    "knnr = KNeighborsRegressor()\n",
    "knnr.fit(x_train_scaled,y_train)\n",
    "\n",
    "# SVM\n",
    "svmr = SVR()\n",
    "svmr.fit(x_train_scaled,y_train)\n",
    "\n",
    "# Regressão por Árvore de Decisão\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(x_train_scaled,y_train)\n",
    "\n",
    "# Regressão por Random\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Regressõ por GB\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Multilayer Perceptron\n",
    "mlpr =  MLPRegressor(random_state=42)\n",
    "mlpr.fit(x_train_scaled,y_train)\n",
    "\n",
    "regressores = {\n",
    "    'LR':lr,\n",
    "    'KNNR':knnr,\n",
    "    'SVMR':svmr,\n",
    "    'RFR':rfr,\n",
    "    'GBR':gbr,\n",
    "    'MLPR':mlpr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83c294",
   "metadata": {},
   "source": [
    "### 1 - Quinto passo: avaliar o desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993444ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados={}\n",
    "resultados_MAE={}\n",
    "resultados_R2={}\n",
    "resultados_MADMAE={}\n",
    "mad = y_test.mad()\n",
    "\n",
    "for rg_name, rg in regressores.items():\n",
    "    \n",
    "    y_pred = rg.predict(x_test_scaled)        # Entrando os dados de teste no modelo ML\n",
    "    mae = mean_absolute_error(y_test, y_pred) # Calculando métrica MAE\n",
    "    r2 = r2_score(y_test, y_pred)             # Calculando métrica R2\n",
    "    \n",
    "    # Salvando resultados\n",
    "    scoring = {'MAE': mae,\n",
    "               'R2' :r2,\n",
    "               'MAD:MAE':mad/mae\n",
    "         }\n",
    "    resultados[rg_name]=scoring\n",
    "    resultados_MAE[rg_name]=mae\n",
    "    resultados_R2[rg_name]=r2\n",
    "    resultados_MADMAE[rg_name]=mad/mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_teste_regressao = pd.DataFrame(data=resultados)\n",
    "resultado_teste_regressao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78358a",
   "metadata": {},
   "source": [
    "### 1 - Extra: Calculando o modelo físico\n",
    "\n",
    "Iremos fazer o cálculo pontual do valor de $v_{med}$ para cada dado experimental, seguindo:\n",
    "\n",
    "### $v_{med}=\\frac{1}{2}\\sqrt{\\frac{2gh}{1+\\beta}}$\n",
    "\n",
    "\n",
    "Vamos definir uma função para isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontra_velocidade(altura, objeto0, objeto1, objeto2):\n",
    "    g=9.8 \n",
    "    # O MF sabe o beta devido a teoria\n",
    "    if objeto0==1:\n",
    "        beta=2/5 # Esfera\n",
    "    elif objeto1==1:\n",
    "        beta=1/2 # Cilindro\n",
    "    elif objeto2==1:\n",
    "        beta=1 # Aro\n",
    "    vel_med = (0.5)*(2*g*altura/(1+beta))**0.5\n",
    "    return vel_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = x_test.copy() \n",
    "x_test2['Vel MF'] = x_test2.apply(lambda x: encontra_velocidade(x['Altura (m)'], x['objeto_esfera'],\n",
    "                                                            x['objeto_cilindro'], x['objeto_aro']), axis=1)\n",
    "y_pred_modelo = np.array(x_test2['Vel MF'])\n",
    "\n",
    "resultados_MAE['MF'] = mean_absolute_error(y_test, y_pred_modelo)\n",
    "resultados_R2['MF'] = r2_score(y_test, y_pred_modelo)\n",
    "resultados_MADMAE['MF'] = mad/resultados_MAE['MF']\n",
    "resultados['MF'] = {'MAE': resultados_MAE['MF'],\n",
    "                    'R2' :resultados_R2['MF'],\n",
    "                    'MAD:MAE' :resultados_MADMAE['MF']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_teste_regressao = pd.DataFrame(data=resultados)\n",
    "resultado_teste_regressao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a73fe",
   "metadata": {},
   "source": [
    "Comparando os desempenhos dos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= knnr.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a436a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura Regressor\n",
    "fig, axd = plt.subplot_mosaic([['(a)', '(c)', '(d)'],\n",
    "                               ['(b)', '(c)', '(d)']],\n",
    "                              figsize=(11, 4.5), constrained_layout=True)\n",
    "\n",
    "\n",
    "# Gráficos de barras (a) e (b)\n",
    "clrs = ['#81BC82' for i in regressores]+['#d5e6ac'] # Definição de cores em código hexadecimal\n",
    "sns.barplot(x=list(resultados_MAE.keys()), y =[round(resultados_MAE[k],3) for k in resultados_MAE.keys()],\n",
    "            ax=axd['(a)'], palette=clrs, edgecolor='grey')\n",
    "sns.barplot(x=list(resultados_R2.keys()),  y =[round(resultados_R2[k],3) for k in resultados_R2.keys()],\n",
    "            ax=axd['(b)'], palette=clrs, edgecolor='grey')\n",
    "\n",
    "axd['(b)'].set_xlabel('Algoritmos de Regressão')\n",
    "axd['(a)'].set_ylabel('MAE (m/s)'), axd['(a)'].set_ylim([0,0.06])\n",
    "axd['(b)'].set_ylabel('$R^2$'), axd['(b)'].set_ylim([0,1])\n",
    "axd['(a)'].set_yticks([0, 0.04, 0.08])\n",
    "axd['(b)'].set_yticks([0, 0.5, 1])\n",
    "axd['(b)'].bar_label(axd['(b)'].containers[0],  rotation=90, label_type='center', color='white')\n",
    "axd['(a)'].set_title('Métricas')\n",
    "plt.setp(axd['(a)'].get_xticklabels(), visible=False)\n",
    "axd['(b)'].tick_params(axis='x', rotation=45)\n",
    "\n",
    "pos=0.05\n",
    "colors_mae=['w','k','w','k','k','w','w']  # Cores dos valores expressos nas barras do item (a)\n",
    "y_pos_mae=[0.027,0.038,0.033,0.038,0.038,0.027,0.04] # Altura dos valores expressos nas abrras do item (a)\n",
    "for i, kp in enumerate([round(resultados_MAE[k],3) for k in resultados_MAE.keys()]):       \n",
    "    axd['(a)'].text(pos,y_pos_mae[i],\n",
    "             s =  '{0:.3f}'.format(kp),\n",
    "             color=colors_mae[i],\n",
    "             rotation=90,\n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='top',\n",
    "             multialignment='center')\n",
    "    pos = pos+1\n",
    "\n",
    "\n",
    "# Grafico de dispersão ML\n",
    "sns.scatterplot(x=y_test,y=y_pred, label='$y_{pred}$', color='#81BC82', edgecolor='k',\n",
    "                marker='o', s=25, ax=axd['(c)'])\n",
    "axd['(c)'].plot([0,1], [0,1], color='r', linestyle='dashed', linewidth = 1,\n",
    "                  label='$y_{pred}=y_{test}$') # Reta 100% correto\n",
    "axd['(c)'].set(xlabel='Velocidade Média \\n Experimental (m/s)', ylabel='Velocidade Média Predita (m/s)')\n",
    "axd['(c)'].legend(loc=4)\n",
    "axd['(c)'].set_xlim([0.2,0.8]), axd['(c)'].set_ylim([0.2,0.8])\n",
    "axd['(c)'].set_xticks([0.2, 0.4, 0.6, 0.8]), axd['(c)'].set_yticks([0.2, 0.4, 0.6, 0.8])\n",
    "axd['(c)'].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "axd['(c)'].yaxis.set_minor_locator(AutoMinorLocator())\n",
    "axd['(c)'].grid(alpha=0.2, linestyle='-.', color='k', linewidth =1)\n",
    "axd['(c)'].set_title('Aprendizado de Máquina')\n",
    "\n",
    "# Grafico de dispersão MF\n",
    "sns.scatterplot(x=y_test,y=y_pred_modelo, label='$y_{pred}$', color='#d5e6ac', edgecolor='k',\n",
    "                marker='o', s=25, ax=axd['(d)'])\n",
    "axd['(d)'].plot([0,1], [0,1], color='r', linestyle='dashed', linewidth = 1,\n",
    "                  label='$y_{pred}=y_{test}$') # Reta 100% correto\n",
    "axd['(d)'].set(xlabel='Velocidade Média \\n Experimental (m/s)', ylabel='Velocidade Média Predita (m/s)')\n",
    "axd['(d)'].legend(loc=4)\n",
    "axd['(d)'].set_xlim([0.2,0.8]), axd['(d)'].set_ylim([0.2,0.8])\n",
    "axd['(d)'].set_xticks([0.2, 0.4, 0.6, 0.8]), axd['(d)'].set_yticks([0.2, 0.4, 0.6, 0.8])\n",
    "axd['(d)'].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "axd['(d)'].yaxis.set_minor_locator(AutoMinorLocator())\n",
    "axd['(d)'].grid(alpha=0.2, linestyle='-.', color='k', linewidth =1)\n",
    "axd['(d)'].set_title('Modelo Físico')\n",
    "axd['(d)'].yaxis.tick_right()\n",
    "axd['(d)'].set_ylabel('')\n",
    "\n",
    "# Escrevendo os itens (a), (b), ... em cada um dos gráficos da figura\n",
    "for label, ax in axd.items():\n",
    "    trans = mtransforms.ScaledTranslation(10/72, -5/72, fig.dpi_scale_trans)\n",
    "    ax.text(0.0, 1.0, label, transform=ax.transAxes + trans,\n",
    "            fontsize='medium', verticalalignment='top')\n",
    "    \n",
    "#plt.savefig(\"regressao.pdf\", format=\"pdf\") # Para salvar a imagem em formato pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e252f64",
   "metadata": {},
   "source": [
    "### 1 - Extra: Validação cruzada\n",
    "\n",
    "Vamos usar a validação cruzada para verificar se o desempenho de cada modelo muda com a partição de treino e teste usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tabela_dados[['Altura (m)','Ângulo (°)', 'objeto_aro', 'objeto_cilindro','objeto_esfera']] \n",
    "y = tabela_dados['Velocidade Média (m/s)'] # Atributo alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ec50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()                          # Regressao Linear\n",
    "knnr = KNeighborsRegressor()                     # KNN Regressor\n",
    "svmr = SVR()                                     # SVM\n",
    "dtr = DecisionTreeRegressor()                    # Regressão por Árvore de Decisão\n",
    "rfr = RandomForestRegressor(random_state=42)     # Regressão por Random\n",
    "gbr = GradientBoostingRegressor(random_state=42) # Regressão por GB\n",
    "mlpr =  MLPRegressor(random_state=42)            # Multilayer Perceptron\n",
    "\n",
    "regressores = {\n",
    "    'LR':lr,\n",
    "    'KNNR':knnr,\n",
    "    'SVMR':svmr,\n",
    "    'RFR':rfr,\n",
    "    'GBR':gbr,\n",
    "    'MLPR':mlpr,\n",
    "    'MF':'encontra_velocidade'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eae34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando 5 folds garantimos usar 20% dos dados para teste e 80% para treinamento\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)    \n",
    "\n",
    "results = []\n",
    "for clf_name, clf in regressores.items():                       # Para cada modelo testado\n",
    "    \n",
    "    r2_list=[]\n",
    "    mae_list=[]\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)): # Para cada fold de dados\n",
    "        \n",
    "        # Separando conjunto de treino e teste\n",
    "        x_train = x.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        x_test = x.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        # Modelo fisico\n",
    "        if clf_name == 'MF':\n",
    "            y_pred = x_test.apply(lambda x: encontra_velocidade(x['Altura (m)'], x['objeto_esfera'],\n",
    "                                                            x['objeto_cilindro'], x['objeto_aro']), axis=1)\n",
    "            \n",
    "            # Avaliando\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Salvando as métricas do fold\n",
    "            mae_list.append(mae)\n",
    "            r2_list.append(r2)\n",
    "            \n",
    "        # Modelos de Machine Learning\n",
    "        else:\n",
    "            \n",
    "            # Escalonando\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(x_train)\n",
    "            x_train_scaled = scaler.transform(x_train)\n",
    "            x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "            # Treinando e predizendo\n",
    "            clf.fit(x_train_scaled,y_train)\n",
    "            y_pred = clf.predict(x_test_scaled)\n",
    "            \n",
    "            # Avaliando\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Salvando as métricas do fold\n",
    "            mae_list.append(mae)\n",
    "            r2_list.append(r2)\n",
    "                \n",
    "    ob = {\n",
    "        'regressor':clf_name,\n",
    "        'mae_avg':np.mean(mae_list),\n",
    "        'mae_std':np.std(mae_list),\n",
    "        'r2_avg':np.mean(r2_list),\n",
    "        'r2_std':np.std(r2_list)\n",
    "    }\n",
    "    results.append(ob)\n",
    "\n",
    "\n",
    "df_reg = pd.DataFrame(data= results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura Validação Cruzada Classificação e Regressão\n",
    "fig, axd = plt.subplot_mosaic([['(a)',],\n",
    "                               ['(b)']],\n",
    "                              figsize=(4, 4), constrained_layout=True)\n",
    "\n",
    "# Gráficos de barras (c) e (d)\n",
    "clrs = ['#81BC82' for i in regressores]+['#d5e6ac'] # Definição de cores em código hexadecimal\n",
    "sns.barplot(x='regressor', y ='mae_avg', data=df_reg,\n",
    "            ax=axd['(a)'], palette=clrs, edgecolor='grey')\n",
    "sns.barplot(x='regressor',  y ='r2_avg', data=df_reg,\n",
    "            ax=axd['(b)'], palette=clrs, edgecolor='grey')\n",
    "\n",
    "axd['(a)'].set_xlabel(''),\n",
    "axd['(b)'].set_xlabel('Algoritmos de Regressão')\n",
    "axd['(a)'].set_ylabel('MAE (m/s)'), axd['(a)'].set_ylim([0,0.1])\n",
    "axd['(b)'].set_ylabel('$R^2$'), axd['(b)'].set_ylim([0,1])\n",
    "axd['(a)'].set_yticks([0, 0.05,  0.1])\n",
    "axd['(b)'].set_yticks([0, 0.5, 1])\n",
    "axd['(a)'].bar_label(axd['(a)'].containers[0],  rotation=90, label_type='edge', color='k', fmt='%.2f')\n",
    "axd['(b)'].bar_label(axd['(b)'].containers[0],  rotation=90, label_type='center', color='white', fmt='%.2f')\n",
    "\n",
    "\n",
    "plt.setp(axd['(a)'].get_xticklabels(), visible=False)\n",
    "axd['(b)'].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "# Barras verticais indicando variabilidade pelo desvio padraõ\n",
    "x_coords = [p.get_x() + 0.5 * p.get_width() for p in axd['(a)'].patches]\n",
    "y_coords = [p.get_height() for p in axd['(a)'].patches]\n",
    "axd['(a)'].errorbar(x=x_coords, y=y_coords, yerr=df_reg['mae_std'], fmt=\"none\", c=\"r\", capsize=0.1)\n",
    "\n",
    "x_coords = [p.get_x() + 0.5 * p.get_width() for p in axd['(b)'].patches]\n",
    "y_coords = [p.get_height() for p in axd['(b)'].patches]\n",
    "axd['(b)'].errorbar(x=x_coords, y=y_coords, yerr=df_reg['r2_std'], fmt=\"none\", c=\"r\", capsize=0.1)\n",
    "\n",
    "\n",
    "#plt.savefig(\"regressao.pdf\", format=\"pdf\") # Para salvar a imagem em formato pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c121f7",
   "metadata": {},
   "source": [
    "### 1 - Extra: regressão de equação predefinida\n",
    "\n",
    "Vamos definir uma função e então usá-la para regredir os dados expeirmentais.\n",
    "\n",
    "A princípio podemos definir qualquer função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raiz(x, a, b):\n",
    "    return np.sqrt(a*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raiz(4, 1, 0) # Estamos passando os coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0a276",
   "metadata": {},
   "source": [
    "Para entrontrar os coeficientes que melhore ajustam a curva a uma série de dados, podems utilizar a biblioteca SicPy. Logo, o que estamos de fato fazendo é uma regressão onde a curva assumida não é mais a equação linear $\\hat{y}=ax+b$ e sim uma expressão genérica que definimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21feb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumindo a queda livre com um ruido gaussiano nos dados de velocidade média coletados\n",
    "# loc=media, scale=desvio padrao\n",
    "\n",
    "x_exemplo = np.linspace(0, 4, 50)\n",
    "y_exemplo = raiz(x_exemplo, 2*10/4, 0) +  np.random.normal(loc=0, scale=0.2, size=x_exemplo.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a14ad8",
   "metadata": {},
   "source": [
    "Regredindo a função definida em raiz com os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(raiz, x_exemplo, y_exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "popt  # Coeficientes estimados e pcov  e a matriz de covariancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_modelo = raiz(x_exemplo, popt[0], popt[1]) # Calculando o modelo encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_exemplo, y_exemplo, 'b*')\n",
    "plt.plot(x_exemplo, y_modelo, 'r-')\n",
    "plt.xlim([0,4]),plt.ylim([0,5])\n",
    "plt.xlabel('Altura [m]'), plt.ylabel('Velocidade média [m/s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e4f9f",
   "metadata": {},
   "source": [
    "No caso do dados reais do plano inclinado, temos um fator complicador: para cada objeto os coeficientes esperados para a função devem ser diferentes. Então, apesar de estarmos \"mais corretos\" no modelo matemático assumido *a priori*, devemos \"adicionar mais física\" ao problema, selecionando cada parcela de dado que será inserida no modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando os objetos\n",
    "dados_aro = tabela_dados[tabela_dados['objeto_aro']==1]\n",
    "dados_esfera = tabela_dados[tabela_dados['objeto_esfera']==1]\n",
    "dados_cilindro = tabela_dados[tabela_dados['objeto_cilindro']==1]\n",
    "\n",
    "# Separando treino e teste\n",
    "x = tabela_dados[['Altura (m)','Ângulo (°)', 'objeto_aro', 'objeto_cilindro','objeto_esfera']] \n",
    "y = tabela_dados['Velocidade Média (m/s)'] # Atributo alvo\n",
    "\n",
    "# Regredindo os dados\n",
    "popt_aro, pcov = curve_fit(raiz, dados_aro['Altura (m)'], dados_aro['Velocidade Média (m/s)']) \n",
    "popt_esfera, pcov = curve_fit(raiz, dados_esfera['Altura (m)'], dados_esfera['Velocidade Média (m/s)']) \n",
    "popt_cilindro, pcov = curve_fit(raiz, dados_cilindro['Altura (m)'], dados_cilindro['Velocidade Média (m/s)'])\n",
    "\n",
    "# Calcuando os modelos regredidos\n",
    "y_aro = raiz(dados_aro['Altura (m)'], popt_aro[0], popt_aro[1])\n",
    "y_esfera = raiz(dados_esfera['Altura (m)'], popt_esfera[0], popt_esfera[1])\n",
    "y_cilindro = raiz(dados_cilindro['Altura (m)'], popt_cilindro[0], popt_cilindro[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3578682",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(8,3))\n",
    "\n",
    "sns.scatterplot(data = dados_aro, x ='Altura (m)', y='Velocidade Média (m/s)',  ax = axs[0], color='b')\n",
    "sns.lineplot(x = dados_aro['Altura (m)'], y = y_aro,  ax = axs[0], color='b')\n",
    "\n",
    "sns.scatterplot(data = dados_esfera, x ='Altura (m)', y='Velocidade Média (m/s)',  ax = axs[1], color='r')\n",
    "sns.lineplot(x = dados_esfera['Altura (m)'], y=y_esfera,  ax = axs[1], color='r')\n",
    "\n",
    "sns.scatterplot(data = dados_cilindro, x ='Altura (m)', y='Velocidade Média (m/s)',  ax = axs[2], color='g')\n",
    "sns.lineplot(x = dados_cilindro['Altura (m)'], y=y_cilindro,  ax = axs[2], color='g')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(popt_aro, popt_esfera, popt_cilindro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_beta(coef_a):\n",
    "    g = 9.8\n",
    "    return (g/(2*coef_a))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beta aro: ', calcula_beta(popt_aro[0]))            # Esperado  B= 1\n",
    "print('Beta esfera: ', calcula_beta(popt_esfera[0]))      # Esperado  B= 2/5 = 0.4\n",
    "print('Beta cilindro: ', calcula_beta(popt_cilindro[0]))  # Esperado  B= 1/2 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50fbfd",
   "metadata": {},
   "source": [
    "### 1 - Extra: Regressão Simbólica\n",
    "\n",
    "Os diferentes métodos de regressão nada mais são do que diferentes estratégias para encontrar coeficientes de funções pré-definidas (métodos paramétricos como regressão linear e SVM) ou para otimizar uma função custo sem conexão direta com o problema (métodos não paramétricos como KNN e Árvore de Decisão).\n",
    "\n",
    "Agora poderíamos estar interessados em um problema mais geral: supor uma expressão que relaciona as entradas em saída, e não apenas os coeficiente que melhor se adequam a uma presuposição original.\n",
    "\n",
    "Neste caso, o \"modelo aprendido\" será exatamente a expressão matemática que melhor relaciona os dados.\n",
    "\n",
    "A tarefa de se regredir expressões matemáticas dá-se o nome de Regressão Simbólica. Vejamos o exemplo sobre todos os dados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e216fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o algorimto de regressao simbolica\n",
    "# Algoritmos Genéticos são usados para reslver esse problema\n",
    "model = SymbolicRegressor(population_size=5000, generations=20, p_crossover=0.7,\n",
    "                          function_set=('add', 'sub', 'mul', 'div', 'sqrt'),\n",
    "                          p_subtree_mutation=0.1, p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                          max_samples=0.9, parsimony_coefficient=0.01, stopping_criteria=0.01,\n",
    "                          verbose=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( np.array(dados_aro['Altura (m)']).reshape(-1,1), dados_aro['Velocidade Média (m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bc252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( np.array(dados_esfera['Altura (m)']).reshape(-1,1), dados_esfera['Velocidade Média (m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2339ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( np.array(dados_cilindro['Altura (m)']).reshape(-1,1), dados_cilindro['Velocidade Média (m/s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92afd1",
   "metadata": {},
   "source": [
    "A expressão encontrada (sqrt) é exatamente aquela que melhor descreve o fenômeno de acordo com os pressupostos físicos.\n",
    "\n",
    "Podemos comparar os coeficientes encontrados pela Regressão Simbólica e pela regressão com função arbitrária $\\sqrt{a\\bf{x}}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_aro = 0.311\n",
    "rs_esfera = 0.226\n",
    "rs_cilindro = 0.251\n",
    "\n",
    "print(1/rs_aro, 1/rs_esfera, 1/rs_cilindro) # Método de regressão simbólica\n",
    "print(popt_aro[0], popt_esfera[0], popt_cilindro[0]) # Método de regressão pre-definida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15ac77",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fa3df",
   "metadata": {},
   "source": [
    "## 2 - MeltingPoint Dataset\n",
    "\n",
    "#### Descrição geral:\n",
    "São fornecidos dados de compostos orgânicos e seus respectivos pontos de fusão (temperatura de transição entre forma sólida e líquida).\n",
    "\n",
    "#### Objetivo:\n",
    "Prever a temperatura de fusão baseado nas cacaterísticas observadas.\n",
    "\n",
    "#### Features (variáveis de entrada):\n",
    "São fornecidos 202 descritores das moléculas em 2D e 3D.\n",
    "\n",
    "#### Alvo (valor de saída):\n",
    "- MTP: temperatura de fusão. \n",
    "\n",
    "#### Referências:\n",
    "- M. Karthikeyan, Robert C. Glen e Andreas Bender, General Melting Point Prediction Based on a Diverse Compound Data Set and Artificial Neural Networks, https://pubs.acs.org/doi/10.1021/ci0500132.\n",
    "- D. Krstajic, L. J Buturovic, D. E. Leahy, e S. Thomas, Cross-validation pitfalls when selecting and assessing regression and classification models, https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas utilizadas\n",
    "\n",
    "import pandas as pd  # Biblioteca para trabalhar com dados na forma de tabela\n",
    "import numpy as np   # Trabalhar com matrizes e vetores\n",
    "import time          # Para medir tempo\n",
    "\n",
    "# Bibliotecas para gerar gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aprendizado de Máquina\n",
    "from sklearn.model_selection import train_test_split   # Separação treino/teste\n",
    "from sklearn.preprocessing import MinMaxScaler         # Escalonador\n",
    "\n",
    "# Algoritmos de Regressão\n",
    "from sklearn.linear_model import LinearRegression      # Regressão Linear\n",
    "from sklearn.linear_model import Ridge                 # Regressão Ridge\n",
    "from sklearn.cross_decomposition import PLSRegression  # Partial-least Squares (PLS)\n",
    "from sklearn.svm import SVR                            # Regressão por Máquina de Vetor Suporte (SVM)\n",
    "from sklearn.tree import DecisionTreeRegressor         # Regressão por Árvore de Decisão\n",
    "from sklearn.neighbors import KNeighborsRegressor      # k-vizinhos mais próximos (KNN)\n",
    "from sklearn.ensemble import RandomForestRegressor     # RandomForest (RF)\n",
    "from sklearn.ensemble import GradientBoostingRegressor # GradientBoosting (GB)\n",
    "from sklearn.neural_network import MLPRegressor        # Multilayer Perceptron\n",
    "\n",
    "# Validação cruzada\n",
    "from sklearn.model_selection import KFold            # Para separar os dados em k folds na regressao\n",
    "from sklearn.model_selection import cross_validate   # Para rodar treinamento e teste sobre kfolds\n",
    "from sklearn import preprocessing          # Auxilia na transformação dos dados (passo 3)\n",
    "from sklearn.pipeline import make_pipeline # Permite realizar uma sequência de processamentos\n",
    "\n",
    "# Métricas de desempenho\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score       # Métricas de Regressão\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455220b9",
   "metadata": {},
   "source": [
    "### 2 - Primeiro passo: carregar os dados e entendê-los\n",
    "\n",
    "Os dados brutos estão no arquivo <code>4137_185_92_DataSetMTPSMIDescr.txt</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://pubs.acs.org/doi/suppl/10.1021/ci0500132/suppl_file/ci0500132si20050112_060506.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip ci0500132si20050112_060506.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp = pd.read_csv('4137_185_92_DataSetMTPSMIDescr.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8a829",
   "metadata": {},
   "source": [
    "Agora vamos renomear a coluna <code>\\$Field_2</code> para <code>MTP</code> e a coluna <code>\\$Field_1</code> para <code>Molecule</code> para facilitar nossa identificação.\n",
    "\n",
    "A coluna <code>SMILE_Molecule</code> tem a representação SMILES da molécula considerada (uma representação visual da molécula pode ser obtida em https://cdb.ics.uci.edu/cgibin/Smi2DepictWeb.py).\n",
    "\n",
    "A coluna <code>MTP</code> é a temperatura de fusão em °C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.rename(columns={'$Field_2': 'MTP'}, inplace=True)\n",
    "df_mp.rename(columns={'$Field_1': 'SMILE_Molecule'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a390d",
   "metadata": {},
   "source": [
    "Temos um total de 4450 materiais. Entretanto, não temos todos os dados (valores das colunas) para todos eles. Então, vamos deletar as linhas que possuem pelo menos um valor vazio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp2 = df_mp.dropna() # Por padrão o dropna() deleta linhas vazias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mp), len(df_mp2) # Perdemos cerca de 50 materiais para garantir que todos teram todas as características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60ebb1",
   "metadata": {},
   "source": [
    "Agora vamos ver se existem dados duplicados. Consideraremos duplicados apenas aqueles que tem SMILES iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp3 = df_mp2.drop_duplicates(subset=['SMILE_Molecule'], keep=False)\n",
    "len(df_mp), len(df_mp2), len(df_mp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7869b",
   "metadata": {},
   "source": [
    "Pelo artigo original dos dados, a coluna <code>Case</code> representa o grupo de dados, sendo <code>Case=0</code> o grupo de 4173 estruturas exatraídas do Molecular Diversity Preservation International (MDPI) database e <code>Case=1</code> referem-se a 277 fármacos extraídos do Merck Index e compilados por Bergstrom et al. \n",
    "\n",
    "Vamos ver as contagens reais desses agrupamentos agora que deletamos uma parte dos materiais considerados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f85f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp3.groupby('Case').count()['SMILE_Molecule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuições da temperatura de fusão\n",
    "fig, axs = plt.subplots(1,2, figsize=(7,3))\n",
    "sns.histplot(ax=axs[0], data=df_mp3, hue='Case', x='MTP', kde=True)\n",
    "sns.boxplot(ax=axs[1], data=df_mp3, x='Case', y='MTP')\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db7934",
   "metadata": {},
   "source": [
    "De vez usar o <code>Case=1</code> como <code>test_set</code> como foi feito no artigo original de Karthikeyan et al., ou considerar apenas o dataset do <code>Case=0</code> tanto como <code>train_set</code> quanto como <code>test_set</code> conforme realizado por Krstajic et al., iremos assumir que ambos os casos são representativos de um mesmo fenômeno (a temperatura de fusão de moléculas orgânicas). Considerando tudo como um único dataset, iremos dividir os conjuntos de treino e teste de maneira aleatória, seguindo metodologia de 80% para dados de treino e 20% para dados de teste:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf0b91",
   "metadata": {},
   "source": [
    "### 2 - Segundo passo: separar os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173404",
   "metadata": {},
   "source": [
    "Temos originalmente 202 descritores (atributos descritivos ou <code>features</code>), considerando que <code>MTP</code> é nossa variável alvo, <code>SMILE_Molecule</code> é uma string que não pode ser interpretada pelo nosso modelo e <code>Case</code> é uma variável com um único valor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_mp3.drop(columns=['Case','MTP', 'SMILE_Molecule']) # Estamos tirando a coluna de grupo e a coluna alvo \n",
    "y = df_mp3['MTP'] # Atributo alvo\n",
    "\n",
    "# Dividindo conjunto de treinamento e conjunto de teste\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d169a7",
   "metadata": {},
   "source": [
    "### 2 - Terceiro passo: transformação dos dados\n",
    "\n",
    "Vamos escalonar as entradas para que fiquem entre 0 e 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640636be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021da26c",
   "metadata": {},
   "source": [
    "### 2 - Quarto passo: treinando os algoritmos de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressao Linear\n",
    "start_time = time.time()\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_scaled, y_train)\n",
    "print(\"LinearReg Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# Ridge Regression\n",
    "start_time = time.time()\n",
    "rdg = Ridge()\n",
    "rdg.fit(x_train_scaled, y_train)\n",
    "print(\"Ridge Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# Partial least Squares PLS\n",
    "start_time = time.time()\n",
    "pls = PLSRegression()\n",
    "pls.fit(x_train_scaled, y_train)\n",
    "print(\"PLS Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# KNN Regressor\n",
    "start_time = time.time()\n",
    "knnr = KNeighborsRegressor()\n",
    "knnr.fit(x_train_scaled,y_train)\n",
    "print(\"KNN Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# SVM\n",
    "start_time = time.time()\n",
    "svmr = SVR()\n",
    "svmr.fit(x_train_scaled,y_train)\n",
    "print(\"SVM Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# Regressão por Árvore de Decisão\n",
    "start_time = time.time()\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(x_train_scaled,y_train)\n",
    "print(\"DT Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# Regressão por RandomForest\n",
    "start_time = time.time()\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr.fit(x_train_scaled, y_train)\n",
    "print(\"RF Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# Regressõ por GB\n",
    "start_time = time.time()\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(x_train_scaled, y_train)\n",
    "print(\"GB Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "# Multilayer Perceptron\n",
    "start_time = time.time()\n",
    "mlpr =  MLPRegressor(random_state=42)\n",
    "mlpr.fit(x_train_scaled,y_train)\n",
    "print(\"MLP Tempo de treinamento: {:.3f} segundos\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385856f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressores = {\n",
    "    'LR':lr,\n",
    "    'RR':rdg,\n",
    "    'PLS':pls,\n",
    "    'KNNR':knnr,\n",
    "    'SVMR':svmr,\n",
    "    'DT':dtr,\n",
    "    'RFR':rfr,\n",
    "    'GBR':gbr,\n",
    "    'MLPR':mlpr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c75ec",
   "metadata": {},
   "source": [
    "### 2 - Quinto passo: avaliar o desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados={}\n",
    "mad_MTP = y_test.mad()\n",
    "\n",
    "for rg_name, rg in regressores.items():\n",
    "    \n",
    "    y_pred = rg.predict(x_test_scaled)        # Entrando os dados de teste no modelo ML\n",
    "    mae = mean_absolute_error(y_test, y_pred) # Calculando métrica MAE\n",
    "    r2 = r2_score(y_test, y_pred)             # Calculando métrica R2\n",
    "    \n",
    "    # Salvando resultados\n",
    "    scoring = {'MAE': mae,\n",
    "               'R2' :r2,\n",
    "               'MAD:MAE':(mad_MTP)/mae\n",
    "         }\n",
    "    resultados[rg_name]=scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_teste_regressao = pd.DataFrame(data=resultados)\n",
    "resultado_teste_regressao.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ad1c2",
   "metadata": {},
   "source": [
    "### 2 - Extra: Validação Cruzada\n",
    "\n",
    "Para verificar a variabilidade inerente a escolha dos dados (entre treinamento e teste), vamos realizar uma validação cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea429a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_mp3.drop(columns=['Case','MTP', 'SMILE_Molecule']) # Estamos tirando a coluna de grupo e a coluna alvo \n",
    "y = df_mp3['MTP'] # Atributo alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e22ce",
   "metadata": {},
   "source": [
    "Modelos já foram criados nas células acima. Basta realizarmos a validação cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # Marcar o tempo de início\n",
    "\n",
    "# Separando 5 folds garantimos usar 20% dos dados para teste e 80% para treinamento\n",
    "# Parâmtro shuffl embaralha os dados antes de dividir os folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)    \n",
    "\n",
    "results = []\n",
    "for clf_name, clf in regressores.items():                       # Para cada modelo testado\n",
    "    \n",
    "    r2_list=[]\n",
    "    mae_list=[]\n",
    "    mad_list=[]\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)): # Para cada fold de dados\n",
    "        \n",
    "        # Separando conjunto de treino e teste\n",
    "        x_train = x.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        x_test = x.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        # Escalonando\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train_scaled = scaler.transform(x_train)\n",
    "        x_test_scaled = scaler.transform(x_test)\n",
    "        \n",
    "        # Treinando e predizendo\n",
    "        clf.fit(x_train_scaled,y_train)\n",
    "        y_pred = clf.predict(x_test_scaled)\n",
    "\n",
    "        # Avaliando\n",
    "        mad = y_test.mad() # Variabilidade intrinseca dos dados de saída\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Salvando as métricas do fold\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "        mad_list.append(mad)\n",
    "     \n",
    "    ob = {\n",
    "        'regressor':clf_name,\n",
    "        'mae_avg':np.mean(mae_list),\n",
    "        'mae_std':np.std(mae_list),\n",
    "        'r2_avg':np.mean(r2_list),\n",
    "        'r2_std':np.std(r2_list),\n",
    "        'mad_avg:mae_avg':np.mean(mad_list)/np.mean(mae_list),\n",
    "    }\n",
    "    results.append(ob)\n",
    "\n",
    "print(\"Tempo de execução da validação cruzada: {:.3f} segundos\".format(time.time() - start_time))\n",
    "df_reg = pd.DataFrame(data= results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_reg.drop(index=0) # Vamos excluir a regressão linear do gráfico final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae16299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura Validação Cruzada Classificação e Regressão\n",
    "fig, axd = plt.subplot_mosaic([['(a)',],\n",
    "                               ['(b)']],\n",
    "                              figsize=(4, 4), constrained_layout=True)\n",
    "\n",
    "# Gráficos de barras (a) e (b)\n",
    "sns.barplot(x='regressor', y ='mae_avg', data=df_reg,\n",
    "            ax=axd['(a)'], color='#81BC82', edgecolor='grey')\n",
    "sns.barplot(x='regressor',  y ='r2_avg', data=df_reg,\n",
    "            ax=axd['(b)'], color='#81BC82', edgecolor='grey')\n",
    "\n",
    "axd['(a)'].set_xlabel(''),\n",
    "axd['(b)'].set_xlabel('Algoritmos de Regressão')\n",
    "axd['(a)'].set_ylabel('MAE (°C)'), axd['(a)'].set_ylim([0,0.1])\n",
    "axd['(b)'].set_ylabel('$R^2$'), axd['(b)'].set_ylim([0,1])\n",
    "axd['(a)'].set_yticks([0, 30,  60])\n",
    "axd['(b)'].set_yticks([0, 0.5, 1])\n",
    "axd['(a)'].bar_label(axd['(a)'].containers[0],  rotation=90, label_type='center', color='white', fmt='%.2f')\n",
    "axd['(b)'].bar_label(axd['(b)'].containers[0],  rotation=90, label_type='center', color='white', fmt='%.2f')\n",
    "\n",
    "\n",
    "plt.setp(axd['(a)'].get_xticklabels(), visible=False)\n",
    "axd['(b)'].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "# Barras verticais indicando variabilidade pelo desvio padraõ\n",
    "x_coords = [p.get_x() + 0.5 * p.get_width() for p in axd['(a)'].patches]\n",
    "y_coords = [p.get_height() for p in axd['(a)'].patches]\n",
    "axd['(a)'].errorbar(x=x_coords, y=y_coords, yerr=df_reg['mae_std'], fmt=\"none\", c=\"r\", capsize=0.1)\n",
    "\n",
    "x_coords = [p.get_x() + 0.5 * p.get_width() for p in axd['(b)'].patches]\n",
    "y_coords = [p.get_height() for p in axd['(b)'].patches]\n",
    "axd['(b)'].errorbar(x=x_coords, y=y_coords, yerr=df_reg['r2_std'], fmt=\"none\", c=\"r\", capsize=0.1)\n",
    "\n",
    "\n",
    "#plt.savefig(\"regressao.pdf\", format=\"pdf\") # Para salvar a imagem em formato pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d883f2b",
   "metadata": {},
   "source": [
    "### 2 - Extra: Retornando para o segundo passo, Seleção de Features\n",
    "\n",
    "Os algoritmos treinados demosntraram capacidade preditiva, entretanto com um baixo desemepnho (gostariamos de R²>0.8 e MAD:MAE>5).\n",
    "\n",
    "Vamos retornar ao segundo passo de **seleção de features** e aplicar alguns métodos para diminuir a quantidade de features usadas. Apesar disso não necessariamente aumentar o desempenho de predição, permite uma **redução de dimensionalidade** dos dados de entrada do algoritmo, o que deve diminuir o tempo de treinamento dos algoritmos.\n",
    "\n",
    "Existem vários métodos de **seleção de features**, como métdos de **filtro**, métodos **Wrapper** e métodos **embutidos**. \n",
    "\n",
    "Os métodos de **filtro** envolvem a seleção prévia das features através de algum critério. Esse critério pode ser de **domínio** (por exemplo, um físico escolhendo os atributos que são relevantes para o fenômeno baseado em seu conhecimento da física do fenômeno); ou podem ser **estatísticos** como eliminar dados correlacionadas (que podem ser redundantes) ou com baixa variância (que não carregam muita informação diferente para cada exemplo).\n",
    "\n",
    "Os métodos **wrapper** podem ser divididos em **Backward elimination** e **Foward selection**, e se baseiam na ideia de selecionar as features através do treinamento de um modelo de aprendizado de máquina selecionado, escolhendo as features usadas a cada etapa de treinando e avaliando uma métrica de desemepenho. Esses métodos são automáticos mas podem gerar overfitting, são custosos em termos de tempo de computação e são dependendes do algoritmo de ML escolhido (features selecionadas para uma RandomForest podem não ser a mesma que para um KNN, por exemplo). \n",
    "\n",
    "Os métodos **embutidos** são detalhes internos do prório algoritmo de aprendizado de máquina. Por exemplo, a Regressão Ridge possui uma penalidade para variáveis correlacionadas na própria função de custo (loss function).\n",
    "\n",
    "No artigo original dos dados aqui estudados (*Karthikeyan et al.*) há uma seleção por filtro de domínio, como as features sendo separadas por aquelas que consideram apenas informações 2D das moléculas e aquelas que consideram informações 3D das moléculas. Entretanto, os autores não fornceram as listas exatas de quais features são de cada um desses grupos.\n",
    "Já no artigo de *Krstajic et al.*, os autores eliminaram 11 features com variação proxima de zero e 22 que eram combinações lineares das outras, restando 169 features. Novamente, contudo, eles não forneceram a lista das features eliminadas ou das restantes.\n",
    "\n",
    "Vamos então adotar critérios próprios para proceder. Iremos utilizar uma combinação dos três métodos de seleção de features descritos acima. Iremos usar uma métrica de **Informação Mútua** para rankear as features que trazer o maior ganho de informação, pegando as **k-primeiras features**. Iremos utilizar o algoritmo de **Regressão Ridge** como nosso algoritmo de teste (pois teve bons resultados em nossa análise preliminar, consome pouco tempo e penaliza variáveis correlacionadas). Vejamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando as k melhores features usando a medida de informação mútua\n",
    "x = df_mp3.drop(columns=['Case','MTP', 'SMILE_Molecule']) # Estamos tirando a coluna de grupo e a coluna alvo \n",
    "y = df_mp3['MTP'] # Atributo alvo\n",
    "\n",
    "# 2 Dividindo conjunto de treinamento e conjunto de teste\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8079046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar os resultados\n",
    "r2_ridge=[]\n",
    "mad_mae_ridge=[]\n",
    "\n",
    "start_time = time.time() # Marcar o tempo de início\n",
    "\n",
    "for k in range(10,201,10):\n",
    "    \n",
    "    # 2 Selecionando as features\n",
    "    selector = SelectKBest(score_func = mutual_info_regression, k=k)\n",
    "    selector.fit(x_train, y_train)\n",
    "    mask = selector.get_support()\n",
    "    new_x_train = x_train[x.columns[mask]] # Estamos selecionando apenas as colunas definidas pelo seletor de features\n",
    "    new_x_test = x_test[x.columns[mask]]   # Estamos selecionando apenas as colunas definidas pelo seletor de features\n",
    "    \n",
    "    # 3 Transformação de Escalonando\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(new_x_train)\n",
    "    x_train_scaled = scaler.transform(new_x_train)\n",
    "    x_test_scaled = scaler.transform(new_x_test)\n",
    "    \n",
    "    # 4 Treinamento Ridge Regression\n",
    "    rdg = Ridge()\n",
    "    rdg.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # 5 Métricas Ridge Regression\n",
    "    y_pred = rdg.predict(x_test_scaled)        \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)             \n",
    "    r2_ridge.append(r2)\n",
    "    mad_MTP = y_test.mad()  # Desvio absoluto médio de y_test\n",
    "    mad_mae_ridge.append(mad_MTP/mae)\n",
    "\n",
    "    #print(k)\n",
    "\n",
    "print(\"Tempo de execução da seleção de features: {:.3f} segundos\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94f04f",
   "metadata": {},
   "source": [
    "Plotando um gráfico do desempenho do algoritmo de em função no número de features utilizadas, podemos ver que o desempenho é somente marginalmente melhor para mais de 100 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i for i in range(10,201,10)]\n",
    "\n",
    "fig, ax1 = plt.subplots( figsize=(5,3))\n",
    "\n",
    "# Plotando a curva de y1 no eixo esquerdo\n",
    "ax1.plot(k, r2_ridge, 'b-')\n",
    "ax1.set_xlabel('Quantidade de Features')\n",
    "ax1.set_ylabel(r'$R^2$', color='b')\n",
    "ax1.set_xlim([0,200])\n",
    "ax1.set_ylim([0.2,0.6])\n",
    "ax1.grid(True)\n",
    "ax2 = ax1.twinx() # Criando o segundo eixo de valores y\n",
    "\n",
    "# Plotando a curva de y2 no eixo direito\n",
    "ax2.plot(k, mad_mae_ridge, 'r-')\n",
    "ax2.set_ylabel('MAD:MAE', color='r')\n",
    "ax2.set_ylim([1.0,1.6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392487d9",
   "metadata": {},
   "source": [
    "### 2 - Extra: Retornando ao quatro passo, Ajuste de hiperparâmetros do modelo\n",
    "\n",
    "Podemos relizar um ajuste fino dos parâmetros de cada algoritmo utilizado. Vamos utilizar a técnica de GridSearch para variar os hiperparâmetros dos algoritmos de Ridge e Multiplayer Perceptron de maneira a encontrar uma solução subótima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b450a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbece8",
   "metadata": {},
   "source": [
    "Vamos selecionar apenas as 100 primeiras features para prosseguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54da998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando as features\n",
    "selector = SelectKBest(score_func = mutual_info_regression, k=100)\n",
    "selector.fit(x_train, y_train)\n",
    "mask = selector.get_support()\n",
    "#x.columns[mask]\n",
    "\n",
    "new_x_train = x_train[x.columns[mask]] # Estamos selecionando apenas as colunas mais importantes\n",
    "new_x_test = x_test[x.columns[mask]] # Atributo alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Transformação de Escalonando\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(new_x_train)\n",
    "x_train_scaled = scaler.transform(new_x_train)\n",
    "x_test_scaled = scaler.transform(new_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Definindo os modelos\n",
    "models = [\n",
    "    Ridge(solver='lsqr'),\n",
    "    MLPRegressor(random_state=42, solver='lbfgs'),\n",
    "    #GradientBoostingRegressor(random_state=42)\n",
    "]\n",
    "\n",
    "# Definindo as grades de parâmetros para busca em grade para cada modelo\n",
    "param_grids = [\n",
    "    {'alpha': [0.01, 0.1, 1, 10, 100]}, # Hiperparametros do Ridge\n",
    "    {'hidden_layer_sizes': [(26,12,), (30,17,), (17,10,), (100,50,30,)],\n",
    "     'activation': ['relu', 'tanh']}, # Hiperparametros do MLP\n",
    "    #{'n_estimators': [50, 100], 'max_depth': [2, 4]} # Hiperparametros do GB\n",
    "]\n",
    "\n",
    "resultados={} # Para armazenar os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando sobre cada modelo e sua respectiva grade de parâmetros\n",
    "for model, param_grid in zip(models, param_grids):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Definindo o objeto GridSearchCV\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    grid.fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # Imprimindo os melhores parâmetros e score\n",
    "    print('Modelo:', model.__class__.__name__)\n",
    "    print('Melhores parâmetros:', grid.best_params_)\n",
    "    print('Menor MAE treino:', -grid.best_score_)\n",
    "    print(\"Tempo de otimização: {:.3f} segundos\".format(time.time() - start_time))\n",
    "\n",
    "    # Fazendo previsões no conjunto de teste\n",
    "    y_pred = grid.predict(x_test_scaled)\n",
    "\n",
    "    # Avaliando o desempenho do modelo no conjunto de teste\n",
    "    mad_MTP = y_test.mad()  # Desvio absoluto médio de y_test\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)     \n",
    "    \n",
    "    # Salvando resultados\n",
    "    rg_name = model.__class__.__name__\n",
    "    # Salvando resultados\n",
    "    scoring = {'MAE': mae,\n",
    "               'R2' :r2,\n",
    "               'MAD:MAE':(mad_MTP)/mae\n",
    "         }\n",
    "    resultados[rg_name]=scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_teste_regressao = pd.DataFrame(data=resultados)\n",
    "resultado_teste_regressao.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treino e teste com os melhores parâmetros\n",
    "ridge = Ridge(solver='lsqr', alpha= 0.01)\n",
    "ridge.fit(x_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(x_test_scaled)\n",
    "\n",
    "mlp = MLPRegressor(random_state=42, solver='lbfgs', activation = 'relu', hidden_layer_sizes=(30, 17,))\n",
    "mlp.fit(x_train_scaled, y_train)\n",
    "y_pred_mlp = mlp.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "sns.scatterplot(y_test, y_pred_ridge, color='b', alpha=0.6,  ax = ax[0])\n",
    "sns.lineplot(y_test, y_test, color='r', ax = ax[0])\n",
    "ax[0].set_title('Regressão Ridge')\n",
    "ax[0].set_ylabel(r'$y_{pred}$ (°C)'), ax[0].set_xlabel(r'$y_{test}$ (°C)')\n",
    "ax[0].set_xlim([0,400]), ax[0].set_ylim([0,400])\n",
    "ax[0].grid(True)\n",
    "\n",
    "sns.scatterplot(y_test, y_pred_mlp, color='g', alpha=0.6,  ax = ax[1])\n",
    "sns.lineplot(y_test, y_test, color='r', ax = ax[1])\n",
    "ax[1].set_title('Regressão MLP')\n",
    "ax[1].set_ylabel(r'$y_{pred}$ (°C)'), ax[1].set_xlabel(r'$y_{test}$ (°C)')\n",
    "ax[1].set_xlim([0,400]), ax[1].set_ylim([0,400])\n",
    "ax[1].grid(True)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a27a",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "## Exercícios propostos\n",
    "\n",
    "1) Reproduzir os preditores de temperatura crítica de supercondutividade reportados em Stanev et al. 2018, https://www.nature.com/articles/s41524-018-0085-8.\n",
    "\n",
    "2) Reproduzir o preditor de energia de bandgap de sólidos inorgânicos reportado em Zhuo et al. 2018, https://pubs.acs.org/doi/10.1021/acs.jpclett.8b00124. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
